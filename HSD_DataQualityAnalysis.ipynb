{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_text(text, preserve_hashtags=False):\n",
    "    text = text.replace('\\n', '')  # remove newline character\n",
    "    text = text.replace('\\r', '')  # remove return character\n",
    "    text = text.replace('NEWLINE_TOKEN', '')  # remove NEWLINE_TOKEN text\n",
    "    words = text.split(' ')\n",
    "    words = [w for w in words if not w.startswith('http')]  # remove links\n",
    "    words = [w for w in words if not w.startswith('@')]  # remove user mentions\n",
    "    if not preserve_hashtags:\n",
    "        words = [w for w in words if not w.startswith('#')]  # remove hashtags\n",
    "    words = [w for w in words if not w.startswith('&') or not w.endswith(';')]  # remove html entities\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def sanitize_label(label):\n",
    "    label = str(label)\n",
    "    label = label.replace('\\n', '')  # remove newline character\n",
    "    label = label.replace('\\r', '')  # remove return character\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cardinality(data):\n",
    "    return {'data': len(data)}\n",
    "\n",
    "def class_frequencies(data):\n",
    "    class_freq = {}\n",
    "    for d in data:\n",
    "        if d[-1] not in class_freq:\n",
    "            class_freq[d[-1]] = 1\n",
    "        else:\n",
    "            class_freq[d[-1]] += 1\n",
    "    return class_freq\n",
    "\n",
    "def class_balance(data):\n",
    "    freq = class_frequencies(data)\n",
    "    total = sum([v for k, v in freq.items()])\n",
    "    return {k: round(float(v)*100/total, 2) for k, v in freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtags(loader, lowercase=False, top_count=200):\n",
    "    data = loader()\n",
    "    cl_hts_dict = {}\n",
    "    \n",
    "    for d in data:\n",
    "        text = d[0].lower() if lowercase else d[0]\n",
    "        hts = re.findall(r\"#(\\w+)\", text)\n",
    "        cl = d[1]\n",
    "        if cl not in cl_hts_dict:\n",
    "            cl_hts_dict[cl] = {}\n",
    "        for ht in hts:\n",
    "            if ht not in cl_hts_dict[cl]:\n",
    "                cl_hts_dict[cl][ht] = 1\n",
    "            else:\n",
    "                cl_hts_dict[cl][ht] += 1\n",
    "    top_hts_dict = {c: sorted(hd.items(), key=lambda x: x[1], reverse=True)[:top_count] for c, hd in cl_hts_dict.items()}\n",
    "    \n",
    "    return top_hts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_quality(loader):\n",
    "    data = loader()\n",
    "    cls = [d[-1] for d in data]\n",
    "    print('Sample data:')\n",
    "    print(data[0])\n",
    "\n",
    "    print('Total: {}'.format(cardinality(data)))\n",
    "    print('Freq: {}'.format(class_frequencies(data)))\n",
    "    print('Balance: {}'.format(class_balance(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poleval 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_poleval2019():\n",
    "    with open('hsd/Poleval2019/train_texts.txt', 'r') as f:\n",
    "        texts = f.readlines()\n",
    "    with open('hsd/Poleval2019/test_texts.txt', 'r') as f:\n",
    "        texts.extend(f.readlines())\n",
    "    \n",
    "    with open('hsd/Poleval2019/train_labels.txt', 'r') as f:\n",
    "        labels = f.readlines()\n",
    "    with open('hsd/Poleval2019/test_labels.txt', 'r') as f:\n",
    "        labels.extend(f.readlines())\n",
    "    \n",
    "    data = [[sanitize_text(t, preserve_hashtags=True), sanitize_label(l)] for t, l in tqdm(zip(texts, labels))]\n",
    "    \n",
    "    with open('hsd/Poleval2019/data.pkl', 'w') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "if not os.path.exists('hsd/Poleval2019/data.pkl'):\n",
    "    preprocess_poleval2019()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_poleval2019():\n",
    "    with open('hsd/Poleval2019/data.pkl', 'r') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "['Dla mnie faworytem do tytu\\xc5\\x82u b\\xc4\\x99dzie Cracovia. Zobaczymy, czy typ si\\xc4\\x99 sprawdzi.', '0']\n",
      "Total: {'data': 11041}\n",
      "Freq: {'1': 278, '0': 10056, '2': 707}\n",
      "Balance: {'1': 2.52, '0': 91.08, '2': 6.4}\n"
     ]
    }
   ],
   "source": [
    "visualize_quality(load_poleval2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_poleval2019()\n",
    "h_list = []\n",
    "for d in data:\n",
    "    words = d[0].split(' ')\n",
    "    h_list += [w for w in words if w.startswith('#')]\n",
    "len(h_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(h_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StormfrontWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stormfrontws():\n",
    "    texts = []\n",
    "    for txt in os.listdir('hsd/StormfrontWS/all_files'):\n",
    "        with open('hsd/StormfrontWS/all_files/' + txt, 'r') as f:\n",
    "            texts.append([txt.replace('.txt', ''), f.read()])\n",
    "    with open('hsd/StormfrontWS/labels.csv', 'r') as f:\n",
    "        labels = list(csv.reader(f))\n",
    "        labels = [[label[0], label[-1]] for label in labels[1:]]\n",
    "    \n",
    "    data = []\n",
    "    for text in tqdm(texts):\n",
    "        cl = filter(lambda l: l[0] == text[0], labels)[0][-1]\n",
    "        data.append([sanitize_text(text[1], preserve_hashtags=True), sanitize_label(cl)])\n",
    "        \n",
    "    with open('hsd/StormfrontWS/data.pkl', 'w') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "if not os.path.exists('hsd/StormfrontWS/data.pkl'):\n",
    "    preprocess_stormfrontws()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stormfrontws():\n",
    "    with open('hsd/StormfrontWS/data.pkl', 'r') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "['I bet all the Ted Pike stuff is on the ADL hit list , look for that to start disappearing down the memory hole .', 'noHate']\n",
      "Total: {'data': 10944}\n",
      "Freq: {'idk/skip': 73, 'noHate': 9507, 'hate': 1196, 'relation': 168}\n",
      "Balance: {'hate': 10.93, 'noHate': 86.87, 'idk/skip': 0.67, 'relation': 1.54}\n"
     ]
    }
   ],
   "source": [
    "visualize_quality(load_stormfrontws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davidson et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_davidson():\n",
    "    with open('hsd/DavidsonEtAl/labeled_data.csv') as f:\n",
    "        raw = list(csv.reader(f))[1:]\n",
    "        data = [[sanitize_text(r[6], preserve_hashtags=True), sanitize_label(r[5])] for r in raw]\n",
    "        \n",
    "    with open('hsd/DavidsonEtAl/data.pkl', 'w') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "if not os.path.exists('hsd/DavidsonEtAl/data.pkl'):\n",
    "    preprocess_davidson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_davidson():\n",
    "    with open('hsd/DavidsonEtAl/data.pkl', 'r') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "[\"!!! RT As a woman you shouldn't complain about cleaning up your house. as a man you should always take the trash out...\", '2']\n",
      "Total: {'data': 24783}\n",
      "Freq: {'1': 19190, '0': 1430, '2': 4163}\n",
      "Balance: {'1': 77.43, '0': 5.77, '2': 16.8}\n"
     ]
    }
   ],
   "source": [
    "visualize_quality(load_davidson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impermium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hsd/Impermium/train.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c1c79d3eb1dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hsd/Impermium/data.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpreprocess_impermium\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-c1c79d3eb1dc>\u001b[0m in \u001b[0;36mpreprocess_impermium\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_impermium\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hsd/Impermium/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msanitize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_hashtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitize_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hsd/Impermium/train.csv'"
     ]
    }
   ],
   "source": [
    "# def preprocess_impermium():\n",
    "#     data = []\n",
    "#     with open('hsd/Impermium/train.csv') as f:\n",
    "#         for row in list(csv.reader(f))[1:]:\n",
    "#             data.append([sanitize_text(row[2], preserve_hashtags=True), sanitize_label(row[0])])\n",
    "#     with open('hsd/Impermium/test.csv') as f:\n",
    "#         for row in list(csv.reader(f))[1:]:\n",
    "#             data.append([sanitize_text(row[2], preserve_hashtags=True), sanitize_label(row[0])])\n",
    "#     with open('hsd/Impermium/verification.csv') as f:\n",
    "#         for row in list(csv.reader(f))[1:]:\n",
    "#             data.append([sanitize_text(row[2], preserve_hashtags=True), sanitize_label(row[1])])\n",
    "    \n",
    "#     with open('hsd/Impermium/data.pkl', 'w') as f:\n",
    "#         pickle.dump(data, f)\n",
    "\n",
    "# if not os.path.exists('hsd/Impermium/data.pkl'):\n",
    "#     preprocess_impermium()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_impermium():\n",
    "#     with open('hsd/Impermium/data.pkl', 'r') as f:\n",
    "#         data = pickle.load(f)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hsd/Impermium/data.pkl'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8190afb71d03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize_quality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_impermium\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-0831f79301d6>\u001b[0m in \u001b[0;36mvisualize_quality\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_quality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sample data:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-de57b858cdfe>\u001b[0m in \u001b[0;36mload_impermium\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_impermium\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hsd/Impermium/data.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hsd/Impermium/data.pkl'"
     ]
    }
   ],
   "source": [
    "# visualize_quality(load_impermium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reddit():\n",
    "    data = []\n",
    "    with open('hsd/Reddit/train.csv') as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            data.append([sanitize_text(row[1], preserve_hashtags=True), sanitize_label(row[2])])\n",
    "    with open('hsd/Reddit/test.csv') as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            data.append([sanitize_text(row[1], preserve_hashtags=True), sanitize_label(row[2])])\n",
    "    \n",
    "    with open('hsd/Reddit/data.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "if not os.path.exists('hsd/Reddit/data.pkl'):\n",
    "    preprocess_reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reddit():\n",
    "    with open('hsd/Reddit/data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample data:\n[\"i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.\", '0']\nTotal: {'data': 55132}\nFreq: {'0': 47053, '1': 8079}\nBalance: {'0': 85.35, '1': 14.65}\n"
     ]
    }
   ],
   "source": [
    "visualize_quality(load_reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxic Comment Classification Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_toxicccc():\n",
    "    equal = lambda a,b: len(a)==len(b) and len(a)==sum([1 for i,j in zip(a,b) if i==j])\n",
    "    data = []\n",
    "    with open('hsd/ToxicCCC/train.csv', 'r') as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            cl = 0 if equal(row[-6:], ['0']*6) else 1\n",
    "    \n",
    "    with open('hsd/ToxicCCC/test_texts.csv', 'r') as f:\n",
    "        with open('hsd/ToxicCCC/test_labels.csv', 'r') as ff:\n",
    "            for row_t, row_l in zip(list(csv.reader(f))[1:], list(csv.reader(ff))[1:]):\n",
    "                if not equal(row_l[-6:], ['-1']*6):\n",
    "                    cl = 0 if equal(row_l[-6:], ['0']*6) else 1\n",
    "                    data.append([sanitize_text(row_t[1], preserve_hashtags=True), sanitize_label(cl)])\n",
    "    \n",
    "    with open('hsd/ToxicCCC/data.pkl', 'w') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "if not os.path.exists('hsd/ToxicCCC/data.pkl'):\n",
    "    preprocess_toxicccc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_toxicccc():\n",
    "    with open('hsd/ToxicCCC/data.pkl', 'r') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "['Thank you for understanding. I think very highly of you and would not revert without discussion.', '0']\n",
      "Total: {'data': 63978}\n",
      "Freq: {'1': 6243, '0': 57735}\n",
      "Balance: {'1': 9.76, '0': 90.24}\n"
     ]
    }
   ],
   "source": [
    "visualize_quality(load_toxicccc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Detox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_wikidetox():\n",
    "    data = []\n",
    "    with open('hsd/WikiDetox/attack_annotated_comments.tsv', 'r') as f:\n",
    "        ann_comments = list(csv.reader(f, delimiter='\\t'))[1:]\n",
    "        with open('hsd/WikiDetox/attack_annotations.tsv', 'r') as ff:\n",
    "            annotations = list(csv.reader(ff, delimiter='\\t'))[1:]\n",
    "            for ac in tqdm(ann_comments):\n",
    "                cl = filter(lambda ann: ann[0] == ac[0], annotations)[0][-1]\n",
    "                data.append([sanitize_text(ac[1], preserve_hashtags=True), sanitize_label(cl)])\n",
    "    \n",
    "    with open('hsd/WikiDetox/data.pkl', 'w') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "if not os.path.exists('hsd/WikiDetox/data.pkl'):\n",
    "    preprocess_wikidetox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikidetox():\n",
    "    with open('hsd/WikiDetox/data.pkl', 'r') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "[\"`-This is not ``creative``.  Those are the dictionary definitions of the terms ``insurance`` and ``ensurance`` as properly applied to ``destruction``.  If you don't understand that, fine, legitimate criticism, I'll write up ``three man cell`` and ``bounty hunter`` and then it will be easy to understand why ``ensured`` and ``insured`` are different - and why both differ from ``assured``.The sentence you quote is absolutely neutral.  You just aren't familiar with the underlying theory of strike-back (e.g. submarines as employed in nuclear warfare) guiding the insurance, nor likely the three man cell structure that kept the IRA from being broken by the British.  If that's my fault, fine, I can fix that to explain.  But ther'es nothing ``personal`` or ``creative`` about it.I'm tired of arguing with you.  Re: the other article, ``multi-party`` turns up plenty, and there is more use of ``mutually`` than ``mutual``.  If I were to apply your standard I'd be moving ``Mutual Assured Destruction`` to ``talk`` for not appealing to a Reagan voter's biases about its effectiveness, and for dropping the ``ly``.There is a double standard in your edits.  If it comes from some US history book, like ``peace movement`` or 'M.A.D.' as defined in 1950, you like it, even if the definition is totally useless in 2002 and only of historical interest.  If it makes any even-obvious connection or implication from the language chosen in multiple profession-specific terms, you consider it somehow non-neutral...  Gandhi thinks ``eye for an eye`` describes riots, death penalty, and war all at once, but you don't.  What do you know that Gandhi doesn't?Guess what:  reality is not neutral.  Current use of terms is slightly more controversial.  Neutrality requires negotiation, and some willingness to learn.This is your problem not mine.  You may dislike the writing, fine, that can be fixed.  But disregarding fundamental axioms of philosphy with names that recur in multiple phrases, or failing to make critical distinctions like 'insurance' versus 'assurance' versus 'ensurance' (which are made in one quote by an Air Force general in an in-context quote), is just a disservice to the reader.If someone comes here to research a topic like MAD, they want some context, beyond history.If this is a history book, fine, it's a history book.  But that wasn't what it was claimed to be...`\", '0.0']\n",
      "Total: {'data': 115864}\n",
      "Freq: {'0.0': 96224, '1.0': 19640}\n",
      "Balance: {'0.0': 83.05, '1.0': 16.95}\n"
     ]
    }
   ],
   "source": [
    "visualize_quality(load_wikidetox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poleval 2019\n",
      "1: ['Minela20', 'PiSkomuna']\n",
      "0: ['BezRetuszu', 'NAME', 'TheVoiceOfPoland', 'UstawaDegradacyjna', 'Woronicza17', 'Wypozyczeni', 'dividetourwarsaw', 'tweetme', 'wieszwiecej', 'woronicza17']\n",
      "2: ['BezRetuszu', 'Morawiecki', 'NBAVote', 'PiSkomuna', 'Woronicza17', 'delegalizacja', 'odjebanie', 'pytamboniewiem', 's', 'woronicza17']\n",
      "\n",
      "StormfrontWS\n",
      "hate: []\n",
      "noHate: ['1', '11', '2', '366388', '39', '4', 'post696651', 'post700257', 'x202a', 'x202c']\n",
      "idk/skip: []\n",
      "relation: ['students']\n",
      "\n",
      "Davidson at al.\n",
      "1: ['12288', '128514', '128530', '128553', '128557', '65039', '8217', '8220', '8221', '8230']\n",
      "0: ['128514', '128557', '8217', '8220', '8221', '8230', 'ISIS', 'LosAngeles', 'faggots', 'tcot']\n",
      "2: ['128514', '8217', '8220', '8221', '8230', 'ISIS', 'Yankees', 'hoosiers', 'iubb', 'tcot']\n",
      "\n",
      "Imperium\n",
      "1: ['333333', '38', 'BASED', 'HopeAndAlgae', 'LL', 'cking', 'nWo4Life', 'skitfuckindaddle', 't', 'trollssuck']\n",
      "0: ['1', '2', '3', '333333', 'FuckSummer', 'WeAreInControl', 'iSheep', 'ixzz1yIC3WHXV', 'nWo4Life', 'nervous']\n",
      "\n",
      "Toxic Comment Classification Challenge\n",
      "1: ['1', '17', 'FDK', 'REDIRECT', 'ck', 'en', 'fceb92', 'fdffe7', 'pricks', 'yolo']\n",
      "0: ['000', '000000', '084080', '1', '2', 'F5FFFA', 'FFFFFF', 'It', 'fceb92', 'fdffe7']\n",
      "\n",
      "Wikipedia Detox\n",
      "0.0: ['000', '000000', '084080', '1', '2', 'F5FFFA', 'FFFFFF', 'It', 'fceb92', 'fdffe7']\n",
      "1.0: ['000', '1', '2', '3', 'F5FFFA', 'I', 'REDIRECT', 'The', 'fceb92', 'fdffe7']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = ['Poleval 2019', 'StormfrontWS', 'Davidson at al.', 'Imperium', 'Toxic Comment Classification Challenge', 'Wikipedia Detox']\n",
    "loaders = [load_poleval2019, load_stormfrontws, load_davidson, load_impermium, load_toxicccc, load_wikidetox]\n",
    "\n",
    "for loader, dataset in zip(loaders, datasets):\n",
    "    hts = hashtags(loader, top_count=10)\n",
    "    \n",
    "    print(dataset)\n",
    "    for c, hs in hts.items():\n",
    "        print('{}: {}'.format(c, sorted([h[0] for h in hs])))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}