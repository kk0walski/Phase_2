{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_text(text, preserve_hashtags=False):\n",
    "    text = text.replace('\\n', '')  # remove newline character\n",
    "    text = text.replace('\\r', '')  # remove return character\n",
    "    text = text.replace('NEWLINE_TOKEN', '')  # remove NEWLINE_TOKEN text\n",
    "    words = text.split(' ')\n",
    "    words = [w for w in words if not w.startswith('http')]  # remove links\n",
    "    words = [w for w in words if not w.startswith('@')]  # remove user mentions\n",
    "    if not preserve_hashtags:\n",
    "        words = [w for w in words if not w.startswith('#')]  # remove hashtags\n",
    "    words = [w for w in words if not w.startswith('&') or not w.endswith(';')]  # remove html entities\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def sanitize_label(label):\n",
    "    label = str(label)\n",
    "    label = label.replace('\\n', '')  # remove newline character\n",
    "    label = label.replace('\\r', '')  # remove return character\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cardinality(data):\n",
    "    return {'data': len(data)}\n",
    "\n",
    "def class_frequencies(data):\n",
    "    class_freq = {}\n",
    "    for d in data:\n",
    "        if d[-1] not in class_freq:\n",
    "            class_freq[d[-1]] = 1\n",
    "        else:\n",
    "            class_freq[d[-1]] += 1\n",
    "    return class_freq\n",
    "\n",
    "def class_balance(data):\n",
    "    freq = class_frequencies(data)\n",
    "    total = sum([v for k, v in freq.items()])\n",
    "    return {k: round(float(v)*100/total, 2) for k, v in freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtags(loader, lowercase=False, top_count=200):\n",
    "    data = loader()\n",
    "    cl_hts_dict = {}\n",
    "    \n",
    "    for d in data:\n",
    "        text = d[0].lower() if lowercase else d[0]\n",
    "        hts = re.findall(r\"#(\\w+)\", text)\n",
    "        cl = d[1]\n",
    "        if cl not in cl_hts_dict:\n",
    "            cl_hts_dict[cl] = {}\n",
    "        for ht in hts:\n",
    "            if ht not in cl_hts_dict[cl]:\n",
    "                cl_hts_dict[cl][ht] = 1\n",
    "            else:\n",
    "                cl_hts_dict[cl][ht] += 1\n",
    "    top_hts_dict = {c: sorted(hd.items(), key=lambda x: x[1], reverse=True)[:top_count] for c, hd in cl_hts_dict.items()}\n",
    "    \n",
    "    return top_hts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_quality(loader):\n",
    "    data = loader()\n",
    "    cls = [d[-1] for d in data]\n",
    "    print('Sample data:')\n",
    "    print(data[0])\n",
    "\n",
    "    print('Total: {}'.format(cardinality(data)))\n",
    "    print('Freq: {}'.format(class_frequencies(data)))\n",
    "    print('Balance: {}'.format(class_balance(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poleval 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_poleval2019():\n",
    "    with open('hsd/Poleval2019/train_texts.txt', 'r') as f:\n",
    "        texts = f.readlines()\n",
    "    with open('hsd/Poleval2019/test_texts.txt', 'r') as f:\n",
    "        texts.extend(f.readlines())\n",
    "    \n",
    "    with open('hsd/Poleval2019/train_labels.txt', 'r') as f:\n",
    "        labels = f.readlines()\n",
    "    with open('hsd/Poleval2019/test_labels.txt', 'r') as f:\n",
    "        labels.extend(f.readlines())\n",
    "    \n",
    "    data = [[sanitize_text(t, preserve_hashtags=True), sanitize_label(l)] for t, l in tqdm(zip(texts, labels))]\n",
    "    \n",
    "    with open('hsd/Poleval2019/data.pkl', 'w') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "if not os.path.exists('hsd/Poleval2019/data.pkl'):\n",
    "    preprocess_poleval2019()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_poleval2019():\n",
    "    with open('hsd/Poleval2019/data.pkl', 'rb') as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'utf-8'\n",
    "        data = u.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample data:\n['Dla mnie faworytem do tytułu będzie Cracovia. Zobaczymy, czy typ się sprawdzi.', '0']\nTotal: {'data': 11041}\nFreq: {'0': 10056, '2': 707, '1': 278}\nBalance: {'0': 91.08, '2': 6.4, '1': 2.52}\n"
     ]
    }
   ],
   "source": [
    "visualize_quality(load_poleval2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "data = load_poleval2019()\n",
    "h_list = []\n",
    "for d in data:\n",
    "    words = d[0].split(' ')\n",
    "    h_list += [w for w in words if w.startswith('#')]\n",
    "len(h_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(np.unique(h_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StormfrontWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_stormfrontws():\n",
    "#     texts = []\n",
    "#     for txt in os.listdir('hsd/StormfrontWS/all_files'):\n",
    "#         with open('hsd/StormfrontWS/all_files/' + txt, 'r') as f:\n",
    "#             texts.append([txt.replace('.txt', ''), f.read()])\n",
    "#     with open('hsd/StormfrontWS/labels.csv', 'r') as f:\n",
    "#         labels = list(csv.reader(f))\n",
    "#         labels = [[label[0], label[-1]] for label in labels[1:]]\n",
    "    \n",
    "#     data = []\n",
    "#     for text in tqdm(texts):\n",
    "#         cl = filter(lambda l: l[0] == text[0], labels)[0][-1]\n",
    "#         data.append([sanitize_text(text[1], preserve_hashtags=True), sanitize_label(cl)])\n",
    "        \n",
    "#     with open('hsd/StormfrontWS/data.pkl', 'w') as f:\n",
    "#         pickle.dump(data, f)\n",
    "\n",
    "# if not os.path.exists('hsd/StormfrontWS/data.pkl'):\n",
    "#     preprocess_stormfrontws()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_stormfrontws():\n",
    "#     with open('hsd/StormfrontWS/data.pkl', 'r') as f:\n",
    "#         data = pickle.load(f)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_quality(load_stormfrontws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davidson et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_davidson():\n",
    "    with open('hsd/DavidsonEtAl/labeled_data.csv') as f:\n",
    "        raw = list(csv.reader(f))[1:]\n",
    "        data = [[sanitize_text(r[6], preserve_hashtags=True), sanitize_label(r[5])] for r in raw]\n",
    "        \n",
    "    with open('hsd/DavidsonEtAl/data.pkl', 'w') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "if not os.path.exists('hsd/DavidsonEtAl/data.pkl'):\n",
    "    preprocess_davidson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_davidson():\n",
    "    with open('hsd/DavidsonEtAl/data.pkl', 'rb') as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'utf-8'\n",
    "        data = u.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample data:\n[\"!!! RT As a woman you shouldn't complain about cleaning up your house. as a man you should always take the trash out...\", '2']\nTotal: {'data': 24783}\nFreq: {'2': 4163, '1': 19190, '0': 1430}\nBalance: {'2': 16.8, '1': 77.43, '0': 5.77}\n"
     ]
    }
   ],
   "source": [
    "visualize_quality(load_davidson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impermium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_impermium():\n",
    "#     data = []\n",
    "#     with open('hsd/Impermium/train.csv') as f:\n",
    "#         for row in list(csv.reader(f))[1:]:\n",
    "#             data.append([sanitize_text(row[2], preserve_hashtags=True), sanitize_label(row[0])])\n",
    "#     with open('hsd/Impermium/test.csv') as f:\n",
    "#         for row in list(csv.reader(f))[1:]:\n",
    "#             data.append([sanitize_text(row[2], preserve_hashtags=True), sanitize_label(row[0])])\n",
    "#     with open('hsd/Impermium/verification.csv') as f:\n",
    "#         for row in list(csv.reader(f))[1:]:\n",
    "#             data.append([sanitize_text(row[2], preserve_hashtags=True), sanitize_label(row[1])])\n",
    "    \n",
    "#     with open('hsd/Impermium/data.pkl', 'w') as f:\n",
    "#         pickle.dump(data, f)\n",
    "\n",
    "# if not os.path.exists('hsd/Impermium/data.pkl'):\n",
    "#     preprocess_impermium()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_impermium():\n",
    "#     with open('hsd/Impermium/data.pkl', 'r') as f:\n",
    "#         data = pickle.load(f)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_quality(load_impermium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reddit():\n",
    "    data = []\n",
    "    with open('hsd/Reddit/clean_data.csv') as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            data.append([sanitize_text(row[1], preserve_hashtags=True), sanitize_label(row[2])])\n",
    "    \n",
    "    with open('hsd/Reddit/data.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "if not os.path.exists('hsd/Reddit/data.pkl'):\n",
    "    preprocess_reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reddit():\n",
    "    with open('hsd/Reddit/data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample data:\n[\"i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.\", '1']\nTotal: {'data': 55132}\nFreq: {'1': 19860, '2': 35272}\nBalance: {'1': 36.02, '2': 63.98}\n"
     ]
    }
   ],
   "source": [
    "visualize_quality(load_reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxic Comment Classification Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_toxicccc():\n",
    "#     equal = lambda a,b: len(a)==len(b) and len(a)==sum([1 for i,j in zip(a,b) if i==j])\n",
    "#     data = []\n",
    "#     with open('hsd/ToxicCCC/train.csv', 'r') as f:\n",
    "#         for row in list(csv.reader(f))[1:]:\n",
    "#             cl = 0 if equal(row[-6:], ['0']*6) else 1\n",
    "    \n",
    "#     with open('hsd/ToxicCCC/test_texts.csv', 'r') as f:\n",
    "#         with open('hsd/ToxicCCC/test_labels.csv', 'r') as ff:\n",
    "#             for row_t, row_l in zip(list(csv.reader(f))[1:], list(csv.reader(ff))[1:]):\n",
    "#                 if not equal(row_l[-6:], ['-1']*6):\n",
    "#                     cl = 0 if equal(row_l[-6:], ['0']*6) else 1\n",
    "#                     data.append([sanitize_text(row_t[1], preserve_hashtags=True), sanitize_label(cl)])\n",
    "    \n",
    "#     with open('hsd/ToxicCCC/data.pkl', 'w') as f:\n",
    "#         pickle.dump(data, f)\n",
    "\n",
    "# if not os.path.exists('hsd/ToxicCCC/data.pkl'):\n",
    "#     preprocess_toxicccc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_toxicccc():\n",
    "#     with open('hsd/ToxicCCC/data.pkl', 'r') as f:\n",
    "#         data = pickle.load(f)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_quality(load_toxicccc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Detox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_wikidetox():\n",
    "#     data = []\n",
    "#     with open('hsd/WikiDetox/attack_annotated_comments.tsv', 'r') as f:\n",
    "#         ann_comments = list(csv.reader(f, delimiter='\\t'))[1:]\n",
    "#         with open('hsd/WikiDetox/attack_annotations.tsv', 'r') as ff:\n",
    "#             annotations = list(csv.reader(ff, delimiter='\\t'))[1:]\n",
    "#             for ac in tqdm(ann_comments):\n",
    "#                 cl = filter(lambda ann: ann[0] == ac[0], annotations)[0][-1]\n",
    "#                 data.append([sanitize_text(ac[1], preserve_hashtags=True), sanitize_label(cl)])\n",
    "    \n",
    "#     with open('hsd/WikiDetox/data.pkl', 'w') as f:\n",
    "#         pickle.dump(data, f)\n",
    "\n",
    "# if not os.path.exists('hsd/WikiDetox/data.pkl'):\n",
    "#     preprocess_wikidetox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_wikidetox():\n",
    "#     with open('hsd/WikiDetox/data.pkl', 'r') as f:\n",
    "#         data = pickle.load(f)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_quality(load_wikidetox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Poleval 2019\n",
      "0: ['BezRetuszu', 'NAME', 'TheVoiceOfPoland', 'UstawaDegradacyjna', 'Woronicza17', 'Wypozyczeni', 'dividetourwarsaw', 'tweetme', 'wieszwiecej', 'woronicza17']\n",
      "2: ['BezRetuszu', 'Kropka', 'Morawiecki', 'NBAVote', 'PiSkomuna', 'Woronicza', 'Woronicza17', 'pytamboniewiem', 'słowoNaNiedzielę', 'woronicza17']\n",
      "1: ['Minela20', 'PiSkomuna']\n",
      "\n",
      "Davidson at al.\n",
      "2: ['128514', '8217', '8220', '8221', '8230', 'ISIS', 'Yankees', 'hoosiers', 'iubb', 'tcot']\n",
      "1: ['12288', '128514', '128530', '128553', '128557', '65039', '8217', '8220', '8221', '8230']\n",
      "0: ['128514', '128557', '8217', '8220', '8221', '8230', 'ISIS', 'LosAngeles', 'faggots', 'tcot']\n",
      "\n",
      "Reddit\n",
      "1: ['GIFWAR', 'GabFam', 'InfoWars', 'JobsNotMobs', 'KAG', 'MAGA', 'RedWave', 'SpeakFreely', 'Trump', 'WalkAway']\n",
      "2: ['GIFWAR', 'GabFam', 'KAG', 'MAGA', 'Q', 'QAnon', 'RedWave', 'Trump', 'WWG1WGA', 'WalkAway']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = ['Poleval 2019', 'Davidson at al.', 'Reddit']\n",
    "loaders = [load_poleval2019, load_davidson, load_reddit]\n",
    "\n",
    "for loader, dataset in zip(loaders, datasets):\n",
    "    hts = hashtags(loader, top_count=10)\n",
    "    \n",
    "    print(dataset)\n",
    "    for c, hs in hts.items():\n",
    "        print('{}: {}'.format(c, sorted([h[0] for h in hs])))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}