{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detector - EN - Features extraction for SVM & Dense model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [this notebook](https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/classifier/final_classifier.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "from klepto.archives import dir_archive\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import fasttext\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "import syllables as sylla\n",
    "from pymagnitude import *\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'svm'\n",
    "dim = 10 if MODEL == 'svm' else 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Davidson et al. data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes pre:\n",
    "    0 - hate speech\n",
    "    1 - offensive language\n",
    "    2 - neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('hsd/Reddit/perfect_data.pkl'):\n",
    "    tweets, labels = [], []\n",
    "    with open('hsd/Reddit/labeled_data.csv', 'r') as f:\n",
    "        for d in tqdm(list(csv.reader(f))[1:]):\n",
    "            tweets.append(d[6])  # tweet\n",
    "            labels.append(d[5])  # class\n",
    "    with open('hsd/Reddit/perfect_data.pkl', 'w') as f:\n",
    "        def chcl(c):\n",
    "            return 0 if c=='0' else 1\n",
    "        labels = list(map(chcl, labels))\n",
    "        pickle.dump((tweets, labels), f)\n",
    "else:\n",
    "    with open('hsd/Reddit/perfect_data.pkl', 'rb') as f:\n",
    "        tweets, labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes post:\n",
    "    0 - no hate\n",
    "    1 - hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tweets: 55132\nLabels: 55132\n"
     ]
    }
   ],
   "source": [
    "print('Tweets: {}'.format(len(tweets)))\n",
    "print('Labels: {}'.format(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(\"i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.\",\n",
       "  1),\n",
       " ('This is what the left is really scared of.    https://redd.it/9rfkts', 0),\n",
       " ('That literally looks like a monkey. Why are we supposed to pretend it’s a person bc it’s wearing a red hat?',\n",
       "  0),\n",
       " ('Dumb Cunt', 1),\n",
       " ('It makes you an asshole.', 0)]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "list(zip(tweets[:5], labels[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "sentiment_analyzer = VS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    #hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = re.sub(hashtag_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def pos_tagger(nltk_tag): \n",
    "    if nltk_tag.startswith('J'): \n",
    "        return wordnet.ADJ \n",
    "    elif nltk_tag.startswith('V'): \n",
    "        return wordnet.VERB \n",
    "    elif nltk_tag.startswith('N'): \n",
    "        return wordnet.NOUN \n",
    "    elif nltk_tag.startswith('R'): \n",
    "        return wordnet.ADV \n",
    "    else:           \n",
    "        return None\n",
    "\n",
    "def word_tokenization(tweet):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = word_tokenize(tweet)\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # words = [w for w in words if not w in stop_words]\n",
    "    tags = nltk.pos_tag(words)\n",
    "    # words = [lemmatizer.lemmatize(w[0]) if pos_tagger(w[1]) is None else lemmatizer.lemmatize(w[0], pos_tagger(w[1])) for w in tags]\n",
    "    tags = [x[1] for x in tags]\n",
    "    return words, tags\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = text_string.encode('ascii', 'ignore').decode('ascii')\n",
    "    parsed_text = re.sub(space_pattern, ' ', parsed_text)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text = parsed_text.strip('#')\n",
    "    list_words, tag_list = word_tokenization(parsed_text)\n",
    "    parsed_text = \" \".join(list_words)\n",
    "    tag_str = ' '.join(tag_list)\n",
    "    return parsed_text, tag_str\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    tweet = \" \".join(re.split(\" \", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "# def get_pos_string(tweet):\n",
    "#     text = preprocess(tweet)\n",
    "#     tokens = word_tokenize(text)\n",
    "#     tags = nltk.pos_tag(tokens)\n",
    "#     tag_list = [x[1] for x in tags]\n",
    "#     tag_str = ' '.join(tag_list)\n",
    "    \n",
    "    # return tag_str\n",
    "\n",
    "def pad_words(words, length):\n",
    "    if len(words) >= length:\n",
    "        return words[:length]\n",
    "    else:\n",
    "        additional = length - len(words)\n",
    "        return words + ['EMPTY']*additional\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet)[0] #Get text only\n",
    "    \n",
    "    syllables = sylla.estimate(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59, 1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)), 2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0 if \"rt\" in words else 1\n",
    "    features = [FKRA, FRE, syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised fastText wordtokens training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('hsd/Reddit/fasttext.ft'):\n",
    "    with open('hsd/Reddit/fasttext.ft', 'a') as f:\n",
    "        for t, l in list(zip(tweets, labels)):\n",
    "            text = preprocess(t)[0]\n",
    "            if len(text) > 0:\n",
    "                f.write('__label__{} {}\\n'.format(l, text))\n",
    "\n",
    "# load fasttext model or train & save if none\n",
    "if os.path.exists('hsd/Reddit/fasttext_{}.bin'.format(MODEL)):\n",
    "    ft_model = fasttext.load_model('hsd/Reddit/fasttext_{}.bin'.format(MODEL))\n",
    "else:\n",
    "    ft_model = fasttext.train_supervised('hsd/Reddit/fasttext.ft',\n",
    "                                         lr=0.5, epoch=50, wordNgrams=3, dim=dim)\n",
    "    ft_model.save_model('hsd/Reddit/fasttext_{}.bin'.format(MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordtoken features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordtoken_fts(data):\n",
    "    \n",
    "    sentences_words = []\n",
    "    for d in data:\n",
    "        sentence = preprocess(d)[0]\n",
    "        sentences_words.append(sentence.split(' '))\n",
    "    \n",
    "    opt_length = int(np.median([len(sw) for sw in sentences_words]))\n",
    "    sentences_words = [pad_words(sw, opt_length) for sw in sentences_words]\n",
    "    \n",
    "    ft_vectors = []\n",
    "    for sw in sentences_words:\n",
    "        ft_vector = []\n",
    "        for w in sw:\n",
    "            ft_vector.extend(ft_model[w])\n",
    "        ft_vectors.append(ft_vector)\n",
    "    \n",
    "    return ft_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtoken_features = get_wordtoken_fts(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.009640693,\n",
       " 0.0067152996,\n",
       " 0.0598391,\n",
       " 0.060052037,\n",
       " 0.027157854,\n",
       " -0.07011807,\n",
       " 0.02078991,\n",
       " -0.08584728,\n",
       " 0.012917716,\n",
       " 0.070992336,\n",
       " -0.5416798,\n",
       " 0.29994857,\n",
       " -0.7409683,\n",
       " -0.18130288,\n",
       " -0.42622593,\n",
       " 0.08789844,\n",
       " 0.30140865,\n",
       " -0.018459707,\n",
       " -0.19386247,\n",
       " 0.42800558,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.63417816,\n",
       " 0.4362548,\n",
       " -0.90651417,\n",
       " -0.30027047,\n",
       " -0.5111813,\n",
       " -0.03127592,\n",
       " 0.3363991,\n",
       " -0.17830524,\n",
       " -0.05456752,\n",
       " 0.5377789,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 9.820641,\n",
       " -5.5694637,\n",
       " 12.352773,\n",
       " 4.0737033,\n",
       " 5.9522233,\n",
       " 0.3452986,\n",
       " -4.803345,\n",
       " 1.2176367,\n",
       " 1.8868251,\n",
       " -5.9742723,\n",
       " 3.0824692,\n",
       " -1.768424,\n",
       " 3.9607162,\n",
       " 1.2683365,\n",
       " 1.6864979,\n",
       " 0.06917644,\n",
       " -1.4328097,\n",
       " 0.478131,\n",
       " 0.59928036,\n",
       " -1.7303797,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.044294905,\n",
       " 0.08971748,\n",
       " -0.07666792,\n",
       " -0.034406543,\n",
       " 0.04423418,\n",
       " -0.08887202,\n",
       " 0.060411062,\n",
       " 0.008470563,\n",
       " 0.04903681,\n",
       " 0.030174775,\n",
       " -0.11842141,\n",
       " -0.055567764,\n",
       " 0.02023152,\n",
       " 0.018077375,\n",
       " -0.14670756,\n",
       " 0.054375328,\n",
       " 0.17414418,\n",
       " -0.056357954,\n",
       " -0.016952926,\n",
       " -0.027801987,\n",
       " 0.09331626,\n",
       " -0.18948185,\n",
       " 0.03925938,\n",
       " -0.059232853,\n",
       " -0.029513936,\n",
       " -0.14291048,\n",
       " 0.08068281,\n",
       " -0.050201293,\n",
       " -0.03056176,\n",
       " -0.06647403,\n",
       " 0.049725052,\n",
       " -0.008837471,\n",
       " -0.059458554,\n",
       " 0.08109287,\n",
       " -0.039119285,\n",
       " 0.013016762,\n",
       " -0.0015472019,\n",
       " 0.044914678,\n",
       " 0.050850283,\n",
       " 0.07968221,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.09088325,\n",
       " 0.081737496,\n",
       " -0.03129801,\n",
       " 0.040636543,\n",
       " -0.05475554,\n",
       " 0.04235192,\n",
       " 0.02509038,\n",
       " -0.08590519,\n",
       " -0.09260271,\n",
       " 0.046627324,\n",
       " -0.16149731,\n",
       " 0.10306988,\n",
       " -0.29470104,\n",
       " 0.03548115,\n",
       " 0.033024993,\n",
       " -0.054678816,\n",
       " 0.040863823,\n",
       " -0.055334974,\n",
       " -0.028684847,\n",
       " 0.07253169]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "wordtoken_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised fastText pos training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('hsd/Reddit/fasttext_pos.ft'):\n",
    "    with open('hsd/Reddit/fasttext_pos.ft', 'a') as f:\n",
    "        for t, l in list(zip(tweets, labels)):\n",
    "            f.write('__label__{} {}\\n'.format(l, preprocess(t)[1]))\n",
    "\n",
    "# load fasttext pos model or train & save if none\n",
    "if os.path.exists('hsd/Reddit/fasttext_pos_{}.bin'.format(MODEL)):\n",
    "    ft_pos_model = fasttext.load_model('hsd/Reddit/fasttext_pos_{}.bin'.format(MODEL))\n",
    "else:\n",
    "    ft_pos_model = fasttext.train_supervised('hsd/Reddit/fasttext_pos.ft',\n",
    "                                             lr=0.5, epoch=50, wordNgrams=3, dim=dim)\n",
    "    ft_pos_model.save_model('hsd/Reddit/fasttext_pos_{}.bin'.format(MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech (PoS) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_fts(data):\n",
    "\n",
    "    #Get POS tags for tweets and save as a string\n",
    "    pos_sentences = []\n",
    "    for d in data:\n",
    "        pos_string = preprocess(d)[1]\n",
    "        pos_sentences.append(pos_string)\n",
    "        \n",
    "        \n",
    "    pos_tags = []\n",
    "    for ps in pos_sentences:\n",
    "        pos_tags.append(ps.split(' '))\n",
    "    \n",
    "    opt_length = int(np.median([len(pt) for pt in pos_tags]))\n",
    "    pos_tags = [pad_words(pt, opt_length) for pt in pos_tags]\n",
    "    \n",
    "    ft_vectors = []\n",
    "    for pt in pos_tags:\n",
    "        ft_vector = []\n",
    "        for t in pt:\n",
    "            ft_vector.extend(ft_pos_model[t])\n",
    "        ft_vectors.append(ft_vector)\n",
    "    \n",
    "    return ft_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = get_pos_fts(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[-0.3552974,\n",
       " 0.17120244,\n",
       " 3.0223274,\n",
       " 0.28159243,\n",
       " -1.7013808,\n",
       " -0.9112481,\n",
       " 0.3820561,\n",
       " 0.2949178,\n",
       " -0.68541044,\n",
       " -0.24389629,\n",
       " 0.56111044,\n",
       " -0.22296062,\n",
       " -0.17741354,\n",
       " -0.4266051,\n",
       " 0.49082732,\n",
       " -0.47689247,\n",
       " -0.042696957,\n",
       " -0.7325433,\n",
       " 0.14173636,\n",
       " 0.89051956,\n",
       " -0.3552974,\n",
       " 0.17120244,\n",
       " 3.0223274,\n",
       " 0.28159243,\n",
       " -1.7013808,\n",
       " -0.9112481,\n",
       " 0.3820561,\n",
       " 0.2949178,\n",
       " -0.68541044,\n",
       " -0.24389629,\n",
       " -0.1884019,\n",
       " 0.18592413,\n",
       " 0.99887717,\n",
       " 0.048742313,\n",
       " -0.7376654,\n",
       " -0.6006107,\n",
       " -0.6597169,\n",
       " 0.6042834,\n",
       " 0.08046319,\n",
       " -0.65350616,\n",
       " -0.05441922,\n",
       " -0.09492745,\n",
       " 0.6491668,\n",
       " -0.8366038,\n",
       " 0.26320815,\n",
       " -0.5242677,\n",
       " -0.2310701,\n",
       " -0.33418062,\n",
       " 0.58393586,\n",
       " 0.32620114,\n",
       " 0.042382233,\n",
       " -0.12674437,\n",
       " -0.020608384,\n",
       " 0.37767172,\n",
       " 0.33082724,\n",
       " -0.08046584,\n",
       " -0.47676143,\n",
       " 0.38759142,\n",
       " 0.18853232,\n",
       " -0.7913379,\n",
       " -0.64299756,\n",
       " 0.2933613,\n",
       " 1.1777091,\n",
       " 0.10103384,\n",
       " 0.0038934315,\n",
       " 0.3675759,\n",
       " -0.16113637,\n",
       " 0.6470854,\n",
       " 1.1688254,\n",
       " -0.0013943309,\n",
       " -0.68914145,\n",
       " -0.19697,\n",
       " 0.32705945,\n",
       " 0.26792356,\n",
       " 0.033401813,\n",
       " 0.35274333,\n",
       " -1.1677237,\n",
       " 0.9707335,\n",
       " 0.118317164,\n",
       " -0.816575,\n",
       " -0.3552974,\n",
       " 0.17120244,\n",
       " 3.0223274,\n",
       " 0.28159243,\n",
       " -1.7013808,\n",
       " -0.9112481,\n",
       " 0.3820561,\n",
       " 0.2949178,\n",
       " -0.68541044,\n",
       " -0.24389629,\n",
       " -0.59172475,\n",
       " -0.45522454,\n",
       " -0.25637227,\n",
       " 0.3316974,\n",
       " 0.2405742,\n",
       " -0.16702697,\n",
       " -0.30268383,\n",
       " 0.068891376,\n",
       " -0.014890425,\n",
       " -0.20638794,\n",
       " 0.15224528,\n",
       " 0.43220133,\n",
       " 0.4365075,\n",
       " 0.22934662,\n",
       " -0.66993946,\n",
       " 0.72864825,\n",
       " -0.67486525,\n",
       " 1.1939847,\n",
       " 0.21479762,\n",
       " -0.5695563,\n",
       " 0.042382233,\n",
       " -0.12674437,\n",
       " -0.020608384,\n",
       " 0.37767172,\n",
       " 0.33082724,\n",
       " -0.08046584,\n",
       " -0.47676143,\n",
       " 0.38759142,\n",
       " 0.18853232,\n",
       " -0.7913379,\n",
       " -0.5272352,\n",
       " 0.048662435,\n",
       " 1.0788894,\n",
       " 0.3404641,\n",
       " -0.99644446,\n",
       " 0.6166101,\n",
       " -0.23468453,\n",
       " 0.4773143,\n",
       " -0.5311603,\n",
       " -0.38750765,\n",
       " -0.05441922,\n",
       " -0.09492745,\n",
       " 0.6491668,\n",
       " -0.8366038,\n",
       " 0.26320815,\n",
       " -0.5242677,\n",
       " -0.2310701,\n",
       " -0.33418062,\n",
       " 0.58393586,\n",
       " 0.32620114,\n",
       " 0.34434193,\n",
       " 0.47750813,\n",
       " 0.9116815,\n",
       " -0.15029427,\n",
       " -1.2092214,\n",
       " 0.116256535,\n",
       " -0.6609252,\n",
       " 0.29237518,\n",
       " -0.23065454,\n",
       " -0.3751698,\n",
       " -0.22960876,\n",
       " 0.34722847,\n",
       " 1.0945753,\n",
       " -0.14097889,\n",
       " -0.48646328,\n",
       " 0.049503844,\n",
       " -0.6324796,\n",
       " 1.2709433,\n",
       " 0.26097792,\n",
       " -1.0074751,\n",
       " 0.20542628,\n",
       " -1.0333586,\n",
       " 1.8143275,\n",
       " 0.11705538,\n",
       " -1.177741,\n",
       " -0.070895515,\n",
       " -0.46788058,\n",
       " 2.0118883,\n",
       " -1.1446028,\n",
       " -1.1300771,\n",
       " -0.3552974,\n",
       " 0.17120244,\n",
       " 3.0223274,\n",
       " 0.28159243,\n",
       " -1.7013808,\n",
       " -0.9112481,\n",
       " 0.3820561,\n",
       " 0.2949178,\n",
       " -0.68541044,\n",
       " -0.24389629,\n",
       " -0.22960876,\n",
       " 0.34722847,\n",
       " 1.0945753,\n",
       " -0.14097889,\n",
       " -0.48646328,\n",
       " 0.049503844,\n",
       " -0.6324796,\n",
       " 1.2709433,\n",
       " 0.26097792,\n",
       " -1.0074751]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "pos_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features = np.array([other_features(t) for t in tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 1.1200e+01,  5.3760e+01,  3.4000e+01,  1.5454e+00,  1.1600e+02,\n",
       "         1.2100e+02,  2.2000e+01,  2.2000e+01,  2.2000e+01,  2.4100e-01,\n",
       "         9.7000e-02,  6.6300e-01, -6.2780e-01,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  1.0000e+00],\n",
       "       [ 2.3000e+00,  9.4300e+01,  1.1000e+01,  1.2222e+00,  4.1000e+01,\n",
       "         6.8000e+01,  1.0000e+01,  9.0000e+00,  8.0000e+00,  2.6200e-01,\n",
       "         0.0000e+00,  7.3800e-01, -4.9270e-01,  0.0000e+00,  0.0000e+00,\n",
       "         1.0000e+00,  1.0000e+00],\n",
       "       [ 1.0000e+01,  6.0630e+01,  3.1000e+01,  1.4762e+00,  1.0300e+02,\n",
       "         1.0700e+02,  2.1000e+01,  2.1000e+01,  1.8000e+01,  6.1000e-02,\n",
       "         1.0900e-01,  8.3000e-01,  2.7320e-01,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  1.0000e+00],\n",
       "       [-3.0000e+00,  1.2021e+02,  2.0000e+00,  1.0000e+00,  9.0000e+00,\n",
       "         9.0000e+00,  2.0000e+00,  2.0000e+00,  2.0000e+00,  1.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00, -7.5790e-01,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  1.0000e+00],\n",
       "       [ 2.9000e+00,  8.3330e+01,  7.0000e+00,  1.3999e+00,  2.3000e+01,\n",
       "         2.4000e+01,  5.0000e+00,  5.0000e+00,  5.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  1.0000e+00]])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "other_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(55132, 190)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "np.array(wordtoken_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(55132, 190)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "np.array(pos_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(55132, 17)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "np.array(other_features).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features and feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "features = np.concatenate([wordtoken_features, pos_features, other_features],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(55132, 397)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = dir_archive('hsd/Reddit/X_y_{}'.format(MODEL), {'features': features, 'labels': labels}, serialized=True)\n",
    "archive.dump()\n",
    "del archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}