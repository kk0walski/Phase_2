{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "from klepto.archives import dir_archive\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import fasttext\n",
    "from pymagnitude import *\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n",
    "reasult = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is forbidden\n",
      "This is forbidden\n",
      "This is forbidden\n"
     ]
    }
   ],
   "source": [
    "from prawcore.exceptions import Forbidden\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "def scrapping(user, own_limit):\n",
    "    reasult = pd.DataFrame()\n",
    "    try:\n",
    "        for submission in reddit.subreddit(user).hot(limit=own_limit):\n",
    "            reasult = reasult.append({'user': user, 'text': submission.selftext}, ignore_index=True)\n",
    "    except Forbidden:\n",
    "        print('This is forbidden')\n",
    "    return reasult\n",
    "\n",
    "subreddits = ['DankMemes', 'Imgoingtohellforthis', 'KotakuInAction', 'MensRights', 'MetaCanada', 'MGTOW', 'PussyPass', 'PussyPassDenied', 'Donald', 'TumblrInAction']\n",
    "for subreddit in subreddits:\n",
    "    df = df.append(scrapping(subreddit, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hsd/Reddit/validate_reddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " df['text'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag): \n",
    "    if nltk_tag.startswith('J'): \n",
    "        return wordnet.ADJ \n",
    "    elif nltk_tag.startswith('V'): \n",
    "        return wordnet.VERB \n",
    "    elif nltk_tag.startswith('N'): \n",
    "        return wordnet.NOUN \n",
    "    elif nltk_tag.startswith('R'): \n",
    "        return wordnet.ADV \n",
    "    else:           \n",
    "        return None\n",
    "\n",
    "def word_tokenization(tweet):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = word_tokenize(tweet)\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    tags = nltk.pos_tag(words)\n",
    "    words = [lemmatizer.lemmatize(w[0]) if pos_tagger(w[1]) is None else lemmatizer.lemmatize(w[0], pos_tagger(w[1])) for w in tags]\n",
    "    tags = [x[1] for x in tags]\n",
    "    return words, tags\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = text_string.encode('ascii', 'ignore').decode('ascii')\n",
    "    parsed_text = re.sub(space_pattern, ' ', parsed_text)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text = parsed_text.strip('#')\n",
    "    list_words, tag_list = word_tokenization(parsed_text)\n",
    "    parsed_text = \" \".join(list_words)\n",
    "    tag_str = ' '.join(tag_list)\n",
    "    return parsed_text, tag_str\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    tweet = \" \".join(re.split(\" \", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "# def get_pos_string(tweet):\n",
    "#     text = preprocess(tweet)\n",
    "#     tokens = word_tokenize(text)\n",
    "#     tags = nltk.pos_tag(tokens)\n",
    "#     tag_list = [x[1] for x in tags]\n",
    "#     tag_str = ' '.join(tag_list)\n",
    "    \n",
    "    # return tag_str\n",
    "\n",
    "def pad_words(words, length):\n",
    "    if len(words) >= length:\n",
    "        return words[:length]\n",
    "    else:\n",
    "        additional = length - len(words)\n",
    "        return words + ['EMPTY']*additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_text'] = df['text'].apply(lambda words: preprocess(words)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hsd/Reddit/validate_reddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}